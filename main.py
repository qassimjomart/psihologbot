import asyncio
import logging
import os
import tempfile
import pathlib

from dotenv import load_dotenv
from prompts import PROMPTS_FILE

from aiogram import Bot, Dispatcher, Router, F
from aiogram.filters import CommandStart, Command
from aiogram.types import Message, Voice, PhotoSize
from aiogram.enums import ParseMode
from aiogram.client.default import DefaultBotProperties
from aiogram import BaseMiddleware 
from typing import Callable, Dict, Any, Awaitable


import google.generativeai as genai
from google.generativeai.types import generation_types

load_dotenv()

BOT_TOKEN = os.getenv("BOT_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gemini-1.5-flash-latest")

if not BOT_TOKEN:
    raise ValueError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å BOT_TOKEN")
if not GEMINI_API_KEY:
    raise ValueError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å GEMINI_API_KEY")

# --- –°–ù–ê–ß–ê–õ–ê –û–ü–†–ï–î–ï–õ–ò–ú –í–°–ï –°–ò–°–¢–ï–ú–ù–´–ï –ü–†–û–ú–ü–¢–´ ---
PSYCHOLOGIST_PROMPT = PROMPTS_FILE
IMAGE_SYSTEM_PROMPT = "–¢—ã - —É–º–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢–µ–±–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –û–ø–∏—à–∏ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–æ, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –∏ –ª–∞–∫–æ–Ω–∏—á–Ω–æ, –∫–∞–∫ –µ—Å–ª–∏ –±—ã —Ç—ã –æ–ø–∏—Å—ã–≤–∞–ª –µ–≥–æ –¥—Ä—É–≥—É. –ü–æ–º–æ–≥–∏ —Å –≤–æ–ø—Ä–æ—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∂–∏, –µ—Å–ª–∏ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ. üñºÔ∏è‚ú®"

# --- –ù–ê–ß–ê–õ–û –ö–û–î–ê –î–õ–Ø –ò–°–¢–û–†–ò–ò –°–û–û–ë–©–ï–ù–ò–ô –í –¢–ï–ö–°–¢–û–í–û–ú –ß–ê–¢–ï ---
# 1. –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞. –¢–µ–ø–µ—Ä—å –æ–Ω –±—É–¥–µ—Ç "–ø—Å–∏—Ö–æ–ª–æ–≥–æ–º".
TEXT_CHAT_SYSTEM_PROMPT = PSYCHOLOGIST_PROMPT # <--- –ò–°–ü–û–õ–¨–ó–£–ï–ú –ü–†–û–ú–ü–¢ –ü–°–ò–•–û–õ–û–ì–ê

# 2. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π USER/ASSISTANT –≤ —Ö—Ä–∞–Ω–∏–º–æ–π –∏—Å—Ç–æ—Ä–∏–∏.
MAX_HISTORY_MESSAGES_IN_LIST = 9


class LoggingMiddleware(BaseMiddleware):
    async def __call__(
        self,
        handler: Callable[[Message, Dict[str, Any]], Awaitable[Any]],
        event: Message,
        data: Dict[str, Any]
    ) -> Any:
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ
        user = event.from_user
        user_info = f"ID: {user.id}, Name: {user.full_name}"
        if user.username:
            user_info += f", @{user.username}"

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        content_info = "[–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Å–æ–æ–±—â–µ–Ω–∏—è]"
        if event.text:
            content_info = f"–¢–µ–∫—Å—Ç: '{event.text}'"
        elif event.voice:
            content_info = f"–ì–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {event.voice.duration}—Å)"
        elif event.photo:
            photo = event.photo[-1]
            content_info = f"–§–æ—Ç–æ (—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ: {photo.width}x{photo.height})"

        logging.info(f"–ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç [{user_info}]. {content_info}")

        return await handler(event, data)

# 3. –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–π —á–∞—Ç–æ–≤
chat_histories = {}

# 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Gemini API –∏ –º–æ–¥–µ–ª–∏
try:
    genai.configure(api_key=GEMINI_API_KEY) # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º –æ–¥–∏–Ω —Ä–∞–∑
    
    # –ú–æ–¥–µ–ª—å –°–ü–ï–¶–ò–ê–õ–¨–ù–û –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞ —Å –∏—Å—Ç–æ—Ä–∏–µ–π
    gemini_text_chat_model = genai.GenerativeModel(
        MODEL_NAME,
        system_instruction=TEXT_CHAT_SYSTEM_PROMPT # –¢–µ–ø–µ—Ä—å –∑–¥–µ—Å—å –ø—Ä–æ–º–ø—Ç –ø—Å–∏—Ö–æ–ª–æ–≥–∞
    )
    
    # –ú–æ–¥–µ–ª—å Gemini –¥–ª—è –æ–±—â–∏—Ö –∑–∞–¥–∞—á (–∞—É–¥–∏–æ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
    gemini_general_purpose_model = genai.GenerativeModel(MODEL_NAME)

except Exception as e:
    logging.error(f"–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Gemini –º–æ–¥–µ–ª–µ–π: {e}")
    gemini_text_chat_model = None
    gemini_general_purpose_model = None
# --- –ö–û–ù–ï–¶ –ö–û–î–ê –î–õ–Ø –ò–°–¢–û–†–ò–ò –°–û–û–ë–©–ï–ù–ò–ô –í –¢–ï–ö–°–¢–û–í–û–ú –ß–ê–¢–ï ---

# –î–ª—è –∞—É–¥–∏–æ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ—Ç –∂–µ –ø—Ä–æ–º–ø—Ç –ø—Å–∏—Ö–æ–ª–æ–≥–∞
AUDIO_SYSTEM_PROMPT = PSYCHOLOGIST_PROMPT


router = Router()

@router.message(CommandStart())
async def cmd_start(message: Message):
    await message.answer(
        f"–ü—Ä–∏–≤–µ—Ç, {message.from_user.full_name}!\n"
        "–Ø —Ç–≤–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∫–æ–º–ø–∞–Ω—å–æ–Ω. ü§ñ –†–∞—Å—Å–∫–∞–∂–∏, —á—Ç–æ —Ç–µ–±—è –±–µ—Å–ø–æ–∫–æ–∏—Ç.\n" # –ò–∑–º–µ–Ω–∏–ª –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
        "–ú–æ–∂–µ—à—å –Ω–∞–ø–∏—Å–∞—Ç—å –º–Ω–µ —Ç–µ–∫—Å—Ç (—è –∑–∞–ø–æ–º–Ω—é –Ω–∞—à —Ä–∞–∑–≥–æ–≤–æ—Ä!), –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ –∏–ª–∏ –∫–∞—Ä—Ç–∏–Ω–∫—É."
    )
    chat_id = message.chat.id
    if chat_id in chat_histories:
        del chat_histories[chat_id]
        logging.info(f"–ò—Å—Ç–æ—Ä–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞ –¥–ª—è {chat_id} –æ—á–∏—â–µ–Ω–∞ –ø–æ –∫–æ–º–∞–Ω–¥–µ /start")

@router.message(Command("help"))
async def cmd_help(message: Message):
    await message.answer(
        "–≠—Ç–æ—Ç –±–æ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Gemini AI.\n\n"
        "<b>–ö–æ–º–∞–Ω–¥—ã:</b>\n"
        "/start - –ù–∞—á–∞—Ç—å –¥–∏–∞–ª–æ–≥ (–æ—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞)\n"
        "/help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ\n\n"
        "–Ø –º–æ–≥—É –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏—Å—Ç–æ—Ä–∏–∏), "
        "–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤—ã–µ üé§ –∏ –æ–ø–∏—Å—ã–≤–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏ üñºÔ∏è (–±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏)."
    )

@router.message(F.text)
async def handle_text_message_with_history(message: Message):
    if not gemini_text_chat_model:
        await message.answer("–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–∞—à —Å–µ—Ä–≤–∏—Å AI –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. üòî")
        return

    user_text = message.text
    if not user_text:
        await message.answer("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–∞–∫–æ–π-–Ω–∏–±—É–¥—å —Ç–µ–∫—Å—Ç. ‚úçÔ∏è")
        return

    processing_message = await message.answer("–°–ª—É—à–∞—é –≤–∞—Å... ü§î") # –û–±–Ω–æ–≤–∏–ª —Å–æ–æ–±—â–µ–Ω–∏–µ
    chat_id = message.chat.id

    if chat_id not in chat_histories:
        chat_histories[chat_id] = []
        logging.info(f"–ù–æ–≤–∞—è –∏—Å—Ç–æ—Ä–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è chat_id: {chat_id}")

    chat_histories[chat_id].append({"role": "user", "parts": [{"text": user_text}]})
    
    if len(chat_histories[chat_id]) > MAX_HISTORY_MESSAGES_IN_LIST:
        chat_histories[chat_id] = chat_histories[chat_id][-MAX_HISTORY_MESSAGES_IN_LIST:]

    try:
        chat_session = gemini_text_chat_model.start_chat(history=chat_histories[chat_id][:-1])
        response = await chat_session.send_message_async(chat_histories[chat_id][-1]['parts'])
        await processing_message.delete()

        if response.text:
            response_text = response.text
            chat_histories[chat_id].append({"role": "model", "parts": [{"text": response_text}]})
            await message.answer(response_text)
            
            if len(chat_histories[chat_id]) > MAX_HISTORY_MESSAGES_IN_LIST:
                chat_histories[chat_id] = chat_histories[chat_id][-MAX_HISTORY_MESSAGES_IN_LIST:]
        else:
            if chat_histories[chat_id] and chat_histories[chat_id][-1]["role"] == "user":
                chat_histories[chat_id].pop()
            logging.warning(f"Gemini API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç (chat_id: {chat_id}): {user_text}")
            # –î–æ–±–∞–≤—å—Ç–µ –≤–∞—à—É –ª–æ–≥–∏–∫—É safety_feedback_info —Å—é–¥–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            await message.answer(f"–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. üòî")

    except generation_types.BlockedPromptException as bpe:
        logging.error(f"–¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (—Å –∏—Å—Ç–æ—Ä–∏–µ–π) –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω: {bpe} –¥–ª—è {chat_id}, —Ç–µ–∫—Å—Ç: {user_text}")
        await processing_message.delete()
        if chat_histories[chat_id] and chat_histories[chat_id][-1]["role"] == "user":
            chat_histories[chat_id].pop()
        await message.answer("–ú–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Ñ–∏–ª—å—Ç—Ä —Å—á–µ–ª –≤–∞—à –∑–∞–ø—Ä–æ—Å –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º—ã–º. üôÖ‚Äç‚ôÇÔ∏è")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ Gemini API (—Ç–µ–∫—Å—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π, {chat_id}): {e}", exc_info=True)
        try: await processing_message.delete()
        except Exception: pass
        await message.answer("–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. üòµ‚Äçüí´")


@router.message(F.voice)
async def handle_voice_message(message: Message, bot: Bot):
    if not gemini_general_purpose_model:
        await message.answer("–°–µ—Ä–≤–∏—Å AI –¥–ª—è –∞—É–¥–∏–æ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. üòî")
        return

    voice: Voice = message.voice
    processing_message = await message.answer("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –≥–æ–ª–æ—Å–æ–≤–æ–µ... üé§üéß")
    gemini_file_resource = None

    with tempfile.TemporaryDirectory() as temp_dir_name:
        temp_dir_path = pathlib.Path(temp_dir_name)
        local_ogg_path = temp_dir_path / f"{voice.file_unique_id}.ogg"
        try:
            await bot.download(file=voice.file_id, destination=local_ogg_path)
            if local_ogg_path.stat().st_size == 0:
                await processing_message.edit_text("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –∞—É–¥–∏–æ. üò•")
                return

            gemini_file_resource = await asyncio.to_thread(
                genai.upload_file, path=local_ogg_path, mime_type="audio/ogg" # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ MIME-—Ç–∏–ø –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π
            )
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º AUDIO_SYSTEM_PROMPT (–∫–æ—Ç–æ—Ä—ã–π —Ç–µ–ø–µ—Ä—å PSYCHOLOGIST_PROMPT)
            contents_for_gemini = [AUDIO_SYSTEM_PROMPT, gemini_file_resource]
            response = await gemini_general_purpose_model.generate_content_async(contents_for_gemini)
            await processing_message.delete()

            if response.text:
                await message.answer(response.text)
            else:
                logging.warning(f"Gemini API –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è –∞—É–¥–∏–æ: {voice.file_id}")
                # –î–æ–±–∞–≤—å—Ç–µ –≤–∞—à—É –ª–æ–≥–∏–∫—É safety_feedback_info —Å—é–¥–∞
                await message.answer(f"–ù–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ. üé§‚ùå")
        except generation_types.BlockedPromptException as bpe:
            logging.error(f"–ó–∞–ø—Ä–æ—Å Gemini –¥–ª—è –∞—É–¥–∏–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω: {bpe}")
            await processing_message.delete()
            await message.answer("–§–∏–ª—å—Ç—Ä —Å—á–µ–ª –∞—É–¥–∏–æ –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º—ã–º. üôÖ‚Äç‚ôÇÔ∏è")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ: {e}", exc_info=True)
            try: await processing_message.delete()
            except Exception: pass
            await message.answer("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ. üòµ‚Äçüí´")
        finally:
            if gemini_file_resource and hasattr(gemini_file_resource, 'name'):
                try: await asyncio.to_thread(genai.delete_file, name=gemini_file_resource.name)
                except Exception as e_del: logging.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ Gemini {gemini_file_resource.name}: {e_del}")

@router.message(F.photo)
async def handle_photo_message(message: Message, bot: Bot):
    if not gemini_general_purpose_model:
        await message.answer("–°–µ—Ä–≤–∏—Å AI –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. üòî")
        return

    photo: PhotoSize = message.photo[-1]
    processing_message = await message.answer("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ... üñºÔ∏èüëÄ")
    gemini_file_resource = None

    with tempfile.TemporaryDirectory() as temp_dir_name:
        temp_dir_path = pathlib.Path(temp_dir_name)
        local_jpg_path = temp_dir_path / f"{photo.file_unique_id}.jpg"
        try:
            await bot.download(file=photo.file_id, destination=local_jpg_path)
            if local_jpg_path.stat().st_size == 0:
                await processing_message.edit_text("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. üò•")
                return
            
            gemini_file_resource = await asyncio.to_thread(
                genai.upload_file, path=local_jpg_path, mime_type="image/jpeg"
            )

            contents_for_gemini = [IMAGE_SYSTEM_PROMPT, gemini_file_resource]
            response = await gemini_general_purpose_model.generate_content_async(contents_for_gemini)
            await processing_message.delete()

            if response.text:
                await message.answer(response.text)
            else:
                logging.warning(f"Gemini API –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {photo.file_id}")
                # –î–æ–±–∞–≤—å—Ç–µ –≤–∞—à—É –ª–æ–≥–∏–∫—É safety_feedback_info —Å—é–¥–∞
                await message.answer(f"–ù–µ —Å–º–æ–≥ –æ–ø–∏—Å–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. üñºÔ∏è‚ùå")
        except generation_types.BlockedPromptException as bpe:
            logging.error(f"–ó–∞–ø—Ä–æ—Å Gemini –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω: {bpe}")
            await processing_message.delete()
            await message.answer("–§–∏–ª—å—Ç—Ä —Å—á–µ–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º—ã–º. üôÖ‚Äç‚ôÇÔ∏è")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}", exc_info=True)
            try: await processing_message.delete()
            except Exception: pass
            await message.answer("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. üòµ‚Äçüí´")
        finally:
            if gemini_file_resource and hasattr(gemini_file_resource, 'name'):
                try: await asyncio.to_thread(genai.delete_file, name=gemini_file_resource.name)
                except Exception as e_del: logging.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ Gemini {gemini_file_resource.name}: {e_del}")

async def main():
    default_properties = DefaultBotProperties(parse_mode=ParseMode.HTML)
    bot = Bot(token=BOT_TOKEN, default=default_properties)
    dp = Dispatcher()

    dp.message.middleware(LoggingMiddleware())
    
    dp.include_router(router)
    await bot.delete_webhook(drop_pending_updates=True)
    logging.info("–ë–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    try:
        await dp.start_polling(bot)
    finally:
        await bot.session.close()
        logging.info("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(name)s - %(message)s")
    asyncio.run(main())
